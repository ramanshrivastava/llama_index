# Mini LlamaIndex - Reasoning-Based Learning Approach

## ğŸ¯ Project Overview

**Mini LlamaIndex** is a learning-focused, from-scratch implementation of a RAG (Retrieval-Augmented Generation) framework, inspired by the production [LlamaIndex](https://github.com/run-llama/llama_index) codebase.

### Why This Project?

Building a simplified version of LlamaIndex from scratch helps you:

1. **Deeply understand RAG systems** - Not just use them, but understand *how* and *why* they work
2. **Learn production patterns** - See real-world design patterns (Strategy, Builder, ABC, etc.) in action
3. **Master embeddings & retrieval** - Understand vector similarity, chunking strategies, and retrieval mechanisms
4. **Explore LLM orchestration** - See how to compose LLMs, retrievers, and synthesizers
5. **Appreciate production complexity** - Understand what production frameworks handle (edge cases, optimizations, integrations)

---

## ğŸ“ Approach: Learning-First, Reasoning-Based

This project follows a **historically-grounded, reasoning-based learning framework** inspired by the mini-CPython learning approach. Every commit is a learning experience.

### Core Principles

#### 1. **Architecture Decision Records (ADRs)**

Every significant design decision is documented with:
- **Context**: What problem are we solving?
- **Decision**: What approach did we choose?
- **Rationale**: Why this over alternatives?
- **LlamaIndex Reference**: How does production LlamaIndex handle this?
- **Trade-offs**: What did we gain/lose?
- **Learning Outcomes**: What should you understand after this?

**Example**: [ADR-003: Vector Store Design](docs/adrs/003-vector-store-design.md)

#### 2. **Historical Timeline**

Each commit maps to LlamaIndex's evolution:
- **Commit 1.1** (Schema) â†’ LlamaIndex v0.1.0 (Nov 2022) - Initial data structures
- **Commit 4.2** (VectorStoreIndex) â†’ LlamaIndex v0.2.0 (Dec 2022) - Vector indexing
- **Commit 7.2** (SimpleSummarize) â†’ LlamaIndex v0.4.0 (Mar 2023) - Response synthesis

This helps you understand **when** and **why** features were added to the real framework.

#### 3. **Learning Checkpoints**

After each phase, test your understanding with:
- **Conceptual Quizzes**: Can you explain the design decisions?
- **Hands-On Exercises**: Extend the feature (e.g., add a custom splitter)
- **Debugging Challenges**: Fix intentional bugs
- **Code Reading**: Study equivalent LlamaIndex code
- **Performance Analysis**: Understand bottlenecks and optimizations

#### 4. **Comparative Analysis**

Side-by-side comparison of:
- **Mini-LlamaIndex**: Our simplified implementation
- **Production LlamaIndex**: Real production code
- **Why Different?**: Explicit trade-offs (simplicity vs features)

Example:

| Aspect | Mini-LlamaIndex | Production LlamaIndex | Why Different? |
|--------|-----------------|----------------------|----------------|
| Lines (schema) | ~300 | ~1,408 | We skip image nodes, complex serialization |
| Index types | 2 (Vector, List) | 7+ | Learning focus on core RAG |

#### 5. **Phased Implementation**

9 phases, 20+ commits, each commit is a working, tested state:

```
Phase 1: Foundation      (Schema, Readers)
Phase 2: Transformations (Node Parsers, Embeddings)
Phase 3: Storage         (Vector Store, Document Store)
Phase 4: Indexing        (VectorStoreIndex, ListIndex)
Phase 5: Retrieval       (Retrievers)
Phase 6: LLM & Prompts   (LLM Interface, Templates)
Phase 7: Synthesis       (Response Synthesizers)
Phase 8: Query Engine    (End-to-End Orchestration)
Phase 9: Integration     (Polish, Docs, Examples)
```

---

## ğŸ“Š Scope: What We Build vs What We Skip

### âœ… What We Build (MVP - 5,000 lines)

**Core RAG Pipeline**:
- âœ… Document loading (txt, pdf, md)
- âœ… Text chunking (sentence, token splitters)
- âœ… Embeddings (mock + integration points for real models)
- âœ… Vector storage (in-memory, cosine similarity)
- âœ… Vector indexing (VectorStoreIndex, ListIndex)
- âœ… Retrieval (vector similarity, top-k)
- âœ… Response synthesis (SimpleSummarize, Refine)
- âœ… Query engine (orchestration)
- âœ… LLM interface (mock + integration points)
- âœ… Prompt templates

**Data Structures**:
- âœ… BaseNode, TextNode, Document
- âœ… QueryBundle, Response, NodeWithScore
- âœ… VectorStoreQuery, VectorStoreQueryResult

**Design Patterns**:
- âœ… Abstract Base Classes (ABC)
- âœ… Strategy Pattern (multiple synthesis strategies)
- âœ… Builder Pattern (from_documents, from_defaults)
- âœ… Composition (QueryEngine = Retriever + Synthesizer)

### âŒ What We Skip (Phase 2+ / Advanced)

**Complex Features**:
- âŒ Advanced index types (KnowledgeGraph, PropertyGraph, Tree)
- âŒ Multi-modal (images, audio)
- âŒ Agents and tool calling
- âŒ Chat engines (stateful conversations)
- âŒ Streaming responses
- âŒ Structured output (Pydantic objects from LLM)
- âŒ Complex postprocessing (reranking, MMR)
- âŒ Metadata filtering (advanced queries)
- âŒ Callbacks and instrumentation (observability)

**Infrastructure**:
- âŒ Disk persistence (initially in-memory only)
- âŒ Cloud integrations (Pinecone, Weaviate, etc.)
- âŒ Production optimizations (FAISS, HNSW)
- âŒ Distributed indexing
- âŒ Async/streaming (initially sync only)

**Edge Cases**:
- âŒ Complex error recovery
- âŒ Unicode/encoding edge cases
- âŒ Large document handling (>100MB)
- âŒ Rate limiting, retries

**Rationale**: Focus on **core concepts** deeply rather than **feature breadth**. You can add advanced features once you understand the fundamentals.

---

## ğŸ§© Architecture: Component Overview

### Data Flow (End-to-End RAG)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     INGESTION PHASE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   Documents (PDFs, TXTs, etc.)
           â†“
   [SimpleDirectoryReader]  â† Load documents
           â†“
   Document[]
           â†“
   [SentenceSplitter]       â† Split into chunks
           â†“
   TextNode[] (no embeddings yet)
           â†“
   [Embedding Model]        â† Generate embeddings
           â†“
   TextNode[] (with embeddings)
           â†“
   [VectorStore.add()]      â† Store vectors
   [DocumentStore.add()]    â† Store nodes
           â†“
   VectorStoreIndex (persisted)


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      QUERY PHASE                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   User Query ("What is RAG?")
           â†“
   [QueryBundle]            â† Wrap query
           â†“
   [Embedding Model]        â† Embed query
           â†“
   QueryBundle (with query embedding)
           â†“
   [VectorIndexRetriever]   â† Similarity search
           â†“
   NodeWithScore[] (top-k nodes)
           â†“
   [Postprocessor]          â† Filter/rerank (optional)
           â†“
   NodeWithScore[] (refined)
           â†“
   [ResponseSynthesizer]    â† Generate response
           â†“ (combines query + nodes â†’ prompt)
   [LLM]                    â† Generate answer
           â†“
   Response (text + sources + metadata)
```

### Component Hierarchy

```
BaseComponent (Pydantic base with serialization)
  â”‚
  â”œâ”€ BaseNode (retrievable units)
  â”‚   â”œâ”€ TextNode (text + optional embedding)
  â”‚   â”‚   â””â”€ Document (top-level ingestion unit)
  â”‚   â””â”€ IndexNode (references other nodes)
  â”‚
  â”œâ”€ TransformComponent (nodes â†’ nodes)
  â”‚   â”œâ”€ NodeParser (chunking)
  â”‚   â”‚   â”œâ”€ SentenceSplitter
  â”‚   â”‚   â””â”€ TokenTextSplitter
  â”‚   â””â”€ BaseEmbedding (text â†’ vectors)
  â”‚       â””â”€ MockEmbedding
  â”‚
  â”œâ”€ BaseRetriever (query â†’ nodes)
  â”‚   â”œâ”€ VectorIndexRetriever
  â”‚   â””â”€ ListIndexRetriever
  â”‚
  â”œâ”€ BaseQueryEngine (query â†’ response)
  â”‚   â””â”€ RetrieverQueryEngine
  â”‚
  â”œâ”€ BaseSynthesizer (query + nodes â†’ response)
  â”‚   â”œâ”€ SimpleSummarize
  â”‚   â””â”€ Refine
  â”‚
  â”œâ”€ BaseLLM (prompts â†’ text)
  â”‚   â””â”€ MockLLM
  â”‚
  â”œâ”€ BaseIndex (organize nodes for retrieval)
  â”‚   â”œâ”€ VectorStoreIndex
  â”‚   â””â”€ ListIndex
  â”‚
  â”œâ”€ BasePydanticVectorStore (CRUD for vectors)
  â”‚   â””â”€ SimpleVectorStore
  â”‚
  â””â”€ BaseDocumentStore (CRUD for documents/nodes)
      â””â”€ SimpleDocumentStore
```

---

## ğŸ“‚ Project Structure

```
mini-llama-index/
â”œâ”€â”€ README.md                           # Quick start
â”œâ”€â”€ LEARNING_GUIDE.md                   # Full learning guide (this doc)
â”œâ”€â”€ HISTORICAL_TIMELINE.md              # LlamaIndex evolution
â”œâ”€â”€ pyproject.toml                      # Poetry dependencies
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ adrs/                          # Architecture Decision Records
â”‚   â”‚   â”œâ”€â”€ 001-schema-design.md
â”‚   â”‚   â”œâ”€â”€ 002-pydantic-choice.md
â”‚   â”‚   â”œâ”€â”€ 003-vector-store-design.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ comparisons/                   # Mini vs Production
â”‚   â”‚   â”œâ”€â”€ schema-comparison.md
â”‚   â”‚   â”œâ”€â”€ retrieval-comparison.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â”œâ”€â”€ checkpoints/                   # Learning checkpoints
â”‚   â”‚   â”œâ”€â”€ phase1-checkpoint.md
â”‚   â”‚   â”œâ”€â”€ phase2-checkpoint.md
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ references/                    # External references
â”‚       â”œâ”€â”€ llamaindex-references.md
â”‚       â””â”€â”€ papers.md
â”‚
â”œâ”€â”€ src/mini_llama_index/              # Main source code
â”‚   â”œâ”€â”€ schema.py                      # Core data structures
â”‚   â”œâ”€â”€ settings.py                    # Global settings
â”‚   â”œâ”€â”€ readers/                       # Document readers
â”‚   â”œâ”€â”€ node_parser/                   # Text splitters
â”‚   â”œâ”€â”€ embeddings/                    # Embedding models
â”‚   â”œâ”€â”€ vector_stores/                 # Vector storage
â”‚   â”œâ”€â”€ indices/                       # Indices
â”‚   â”œâ”€â”€ retrievers/                    # Retrievers
â”‚   â”œâ”€â”€ llms/                          # LLM interface
â”‚   â”œâ”€â”€ prompts/                       # Prompt templates
â”‚   â”œâ”€â”€ response_synthesizers/         # Response synthesis
â”‚   â”œâ”€â”€ query_engine/                  # Query engines
â”‚   â””â”€â”€ storage/                       # Storage context
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                          # Unit tests
â”‚   â”œâ”€â”€ integration/                   # End-to-end tests
â”‚   â””â”€â”€ fixtures/                      # Test data
â”‚
â””â”€â”€ examples/                          # Example scripts
    â”œâ”€â”€ 01_basic_indexing.py
    â”œâ”€â”€ 02_simple_query.py
    â””â”€â”€ ...
```

---

## ğŸ“ Learning Methodology

### For Each Phase

1. **Read the Phase Description** in the learning guide
2. **Review the ADR(s)** for that phase (understand *why* before *what*)
3. **Implement the code** following TDD (tests first when possible)
4. **Compare with LlamaIndex** production code (see how they did it)
5. **Complete the checkpoint** (quiz, exercises, debugging)
6. **Commit** with a descriptive message (see commit template below)
7. **Move to next phase**

### Commit Message Template

```
[Phase X.Y] Title - Brief description

Detailed description of what this commit implements.

Design Decisions:
- Decision 1: [rationale]
- Decision 2: [rationale]

Trade-offs:
- We simplified [X] because [Y]
- We kept [A] to preserve [B]

Learning Outcomes:
1. Understand [concept]
2. See how [feature] works

LlamaIndex Reference: [file:line]
ADR: docs/adrs/ADR-XXX.md
Lines added: ~XXX
```

### Example Commit

```
[Phase 3.2] SimpleVectorStore - In-memory vector storage with cosine similarity

Implements an in-memory vector store using numpy for similarity computation.

Design Decisions:
- Use dict-based storage (node_id â†’ embedding) for simplicity
- Cosine similarity as default metric (most common for embeddings)
- Post-filtering for metadata (not during similarity search)

Trade-offs:
- Simplified: No FAISS/HNSW indexing (linear search only)
- Kept: Pluggable similarity metric (easy to extend)

Learning Outcomes:
1. Understand vector similarity search (cosine, dot product)
2. See trade-offs: accuracy vs speed (linear vs indexed search)
3. Learn why metadata filtering is expensive

LlamaIndex Reference: llama-index-core/llama_index/core/vector_stores/simple.py
ADR: docs/adrs/ADR-003-vector-store-design.md
Lines added: ~300
```

---

## ğŸ“Š Success Metrics

### Technical Mastery

After completing Mini-LlamaIndex, you should be able to:

- [ ] **Explain RAG** to a non-technical person
- [ ] **Diagram the data flow** from document â†’ response from memory
- [ ] **Implement a custom component** (splitter, retriever, synthesizer) without guidance
- [ ] **Debug retrieval issues** (e.g., why are my results bad?)
- [ ] **Choose chunking strategies** for different use cases
- [ ] **Optimize embeddings** (batching, caching)
- [ ] **Compare synthesis strategies** (when to use SimpleSummarize vs Refine)
- [ ] **Read LlamaIndex production code** comfortably

### Conceptual Understanding

- [ ] **Why chunking?** (embedding limits, semantic coherence)
- [ ] **Why overlap?** (preserve boundary context)
- [ ] **Why vector similarity?** (semantic search vs keyword search)
- [ ] **Why RAG vs fine-tuning?** (cost, flexibility, freshness)
- [ ] **Why multiple synthesis strategies?** (trade-offs: speed vs quality)
- [ ] **Why metadata?** (filtering, attribution, debugging)

### Engineering Practices

- [ ] **Modular design** (composition over inheritance)
- [ ] **Abstract base classes** (interfaces, protocols)
- [ ] **Builder pattern** (from_documents, from_defaults)
- [ ] **Strategy pattern** (pluggable components)
- [ ] **Testing** (unit, integration, fixtures)
- [ ] **Documentation** (ADRs, docstrings, examples)

---

## ğŸ”— References

### LlamaIndex Production

- **Repository**: https://github.com/run-llama/llama_index
- **Documentation**: https://docs.llamaindex.ai/
- **Code Reference**: `/home/user/llama_index/llama-index-core/`

### Papers

1. [Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2005.11401) - Lewis et al., 2020
2. [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906) - Karpukhin et al., 2020
3. [Self-RAG](https://arxiv.org/abs/2310.11511) - Asai et al., 2023

### Additional Resources

- [Pinecone: Vector Databases](https://www.pinecone.io/learn/vector-database/)
- [LlamaIndex Blog](https://www.llamaindex.ai/blog)
- [Anthropic: Prompt Engineering](https://docs.anthropic.com/claude/docs/prompt-engineering)

---

## ğŸš€ Getting Started

### Prerequisites

- Python 3.10+
- Poetry (or pip with virtualenv)
- Basic knowledge of Python, Pydantic, numpy
- Familiarity with LLMs and embeddings (conceptual)

### Installation

```bash
# Clone this repository
git clone <your-repo>
cd mini-llama-index

# Install dependencies
poetry install

# Run tests
poetry run pytest

# Try first example
poetry run python examples/01_basic_indexing.py
```

### Your First Task

**Implement Phase 1, Commit 1.1: Core Schema**

1. Read `docs/adrs/001-schema-design.md` (you'll write this)
2. Implement `src/mini_llama_index/schema.py`:
   - `BaseComponent`
   - `BaseNode`
   - `TextNode`
   - `Document`
3. Write tests in `tests/unit/test_schema.py`
4. Commit with descriptive message
5. Move to Phase 1, Commit 1.2

---

## ğŸ¯ Final Thoughts

**This is not a typical coding project.** It's a **learning journey** where you:

1. **Build something real** (a working RAG framework)
2. **Understand production systems** (by comparing to LlamaIndex)
3. **Make deliberate decisions** (documented in ADRs)
4. **Learn by doing** (checkpoints, exercises, debugging)

**Focus on depth, not speed.** Take time to:
- Understand *why* before *what*
- Read the production code
- Complete the checkpoints
- Experiment and break things

By the end, you'll have:
- âœ… A working RAG framework (~5,000 lines)
- âœ… Deep understanding of retrieval and generation
- âœ… Production-level design patterns
- âœ… Confidence to build LLM applications

**Good luck, and enjoy the learning process! ğŸ“ğŸš€**

---

## ğŸ“§ Feedback & Contributions

This is a living learning guide. If you:
- Find errors or unclear explanations
- Have suggestions for better learning exercises
- Want to contribute additional ADRs or comparisons

Please open an issue or submit a PR!

---

**Happy Learning! ğŸ¦™ğŸ“š**
